{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Locally (Windows)\n",
    "\n",
    "```powershell\n",
    "$env:PYTHONPATH = \"$PWD\"\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Purpose\n\n**What Shifts:**\n- **From:** M3.1 — Compliance Monitoring Dashboards\n- **To:** M3.2 — Automated Compliance Testing\n\n**Why This Bridge Matters:**\n\nIn M3.1, you built production-grade monitoring that **detects** compliance violations after they've been deployed. Your Prometheus + Grafana dashboards show violations within 15 seconds—but that's still too late.\n\nM3.2 shifts the paradigm from **reactive detection to proactive prevention**. Instead of catching violations after deployment, you'll build automated compliance tests using OPA (Open Policy Agent) that **block** deployments if compliance breaks—preventing 95%+ of violations before they reach production.\n\n**The Critical Gap:** Your M3.1 dashboard caught a PII leak 9 hours after deployment—after 847 users downloaded unredacted SSNs. The result: $2.8M fine, SOC 2 revocation, 3 executives terminated. Monitoring shows problems. It doesn't prevent them.\n\n**Bridge Type:** Readiness Validation\n\nThis bridge validates you have the monitoring foundation in place before building prevention controls on top of it.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Concepts Covered\n\n**New Concepts in M3.2:**\n\n- **Policy-as-Code with OPA Rego** — Write compliance rules as executable code that can be automatically tested, not documentation that can be ignored\n- **Automated Compliance Tests (16+ tests)** — PII detection coverage (100% of text paths), zero cross-tenant leaks, audit logging completeness (>99.5%), SOX 7-year retention verification\n- **CI/CD Integration with Pass/Fail Gates** — Tests run automatically on every commit via GitHub Actions/GitLab CI; deployments blocked if any test fails\n- **Regression Prevention** — Ensure working controls stay working; catch when \"small optimizations\" bypass compliance before they ship\n- **Preventive vs. Detective Controls** — Shift from monitoring violations after deployment to blocking violations before production\n- **Automated SOC 2 Evidence Generation** — Reduce audit prep from 8 hours of manual evidence gathering to 30 minutes of automated export\n\n**Building On:**\n\n- **M3.1 established:** Real-time compliance monitoring (Prometheus + Grafana), 6 critical KPIs tracked, stakeholder dashboards, multi-tenant isolation\n- **M3.2 extends:** From detecting violations (reactive) to preventing violations (proactive) by adding automated testing gates in CI/CD pipelines",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3. After Completing This Bridge\n\n**You Will Be Able To:**\n\n- ✓ Verify that your M3.1 monitoring infrastructure is operational and ready to complement M3.2's prevention layer\n- ✓ Confirm you understand the critical difference between detective controls (monitoring) and preventive controls (automated testing)\n- ✓ Validate that you have the conceptual foundation for policy-as-code and CI/CD integration\n- ✓ Assess whether you're ready to build automated compliance tests that block deployments\n- ✓ Understand why CFOs, Compliance Officers, and CTOs demand preventive controls, not just monitoring\n\n**Pass Criteria:**\n\n- All **5 readiness checks** pass (✓)\n- No critical gaps (✗) in monitoring foundation or conceptual understanding\n- Ready for M3.2 content (OPA Rego policy writing, CI/CD gate implementation)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Context in Track\n\n**Position:** Bridge L3.M3.1 → L3.M3.2\n\n**Learning Journey:**\n\n```\nL3.M3.1                    [THIS BRIDGE]                    L3.M3.2\nCompliance Monitoring  ────  Validation  ────→  Automated Compliance Testing\n(Detective Controls)                            (Preventive Controls)\nReal-time dashboards                            CI/CD testing gates\nDetects violations                              Prevents violations\n```\n\n**Compliance Maturity Progression:**\n\n- **Level 1 (M1-M2):** Manual compliance controls - PII pipelines, audit logging, access controls ✅ Complete\n- **Level 2 (M3.1):** Real-time monitoring - Dashboards showing violations within minutes ✅ Just built\n- **Level 3 (M3.2):** Automated prevention - Tests that block violations before production ← **Next**\n- **Level 4 (Future):** Predictive compliance - ML-based proactive compliance\n\n**Time Estimate:** 15-30 minutes",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Recap: What You Built in M3.1\n\nIn M3.1, you shipped a **production-grade compliance monitoring dashboard** for your 50-tenant GCC deployment.\n\n**Key Deliverables:**\n\n- **Real-time compliance visibility** — Prometheus metrics collection from every RAG component with 15-second refresh intervals\n- **Six critical KPIs tracked:**\n  - Audit trail completeness >99%\n  - PII detection accuracy >99% recall\n  - Access control violations <0.1%\n  - Encryption coverage 100%\n  - Certificate expiry alerts (30 days advance)\n  - Policy compliance score\n- **Stakeholder-specific dashboards:**\n  - CFO sees audit readiness and costs\n  - CTO sees technical health metrics\n  - Compliance Officer sees regulatory adherence and violation trends\n- **Multi-tenant metrics isolation** — Each of 50 business units sees only their compliance posture\n- **13-month data retention** — SOX compliance requirement met\n- **Automated SOC 2 evidence generation** — One-click export of 90-day compliance reports\n\n**What This Solved:** Reduced compliance posture visibility from **3 days → 15 seconds**\n\n**What It Didn't Solve:** Violations still reach production. Your dashboard detects them hours later—after damage is done.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Readiness Check #1: Monitoring Infrastructure Validation\n\n**What This Validates:** Your M3.1 monitoring dashboard is operational and can serve as the foundation for M3.2's prevention layer\n\n**Pass Criteria:**\n- ✓ You have a working Prometheus + Grafana monitoring setup (or equivalent)\n- ✓ You're tracking the 6 critical KPIs (audit trail, PII detection, access control, encryption, certificates, policy compliance)\n- ✓ Your dashboard provides 15-second (or near real-time) refresh intervals\n- ✓ Multi-tenant metrics isolation is functional\n\n**Why This Matters:** M3.2's automated testing will complement (not replace) your monitoring. Prevention catches violations before deployment; monitoring catches anything that slips through. Both layers are required for SOC 2 Type II.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check #1: Monitoring Infrastructure Validation\nprint(\"Check #1: Monitoring Infrastructure Validation\")\nprint(\"=\" * 50)\n\n# Self-assessment questions for monitoring readiness\nquestions = [\n    \"1. Do you have a working Prometheus + Grafana setup (or equivalent monitoring)?\",\n    \"2. Are you tracking these 6 KPIs: audit trail, PII detection, access control, encryption, certificates, policy compliance?\",\n    \"3. Does your dashboard refresh in near real-time (15-30 seconds)?\",\n    \"4. Is multi-tenant metrics isolation working (each tenant sees only their data)?\"\n]\n\nprint(\"\\n✓ Answer 'yes' to all questions to pass this check:\\n\")\nfor q in questions:\n    print(f\"   {q}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"✓ Check #1 PASS if you answered YES to all 4 questions\")\nprint(\"✗ Check #1 FAIL if any answer is NO\")\nprint(\"\\n   Fix: Complete M3.1 PractaThon before proceeding to M3.2\")\n\n# Expected: User self-assesses monitoring infrastructure readiness",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Readiness Check #2: Conceptual Understanding (Detective vs. Preventive Controls)\n\n**What This Validates:** You understand the critical paradigm shift from reactive detection to proactive prevention\n\n**Pass Criteria:**\n- ✓ You can explain the difference between detective controls (monitoring) and preventive controls (automated testing)\n- ✓ You understand why monitoring alone is insufficient for SOC 2 Type II compliance\n- ✓ You can articulate why a 9-hour detection window (even 15 seconds!) is too late\n- ✓ You understand how CI/CD testing gates prevent violations from reaching production\n\n**Why This Matters:** M3.2 requires a mindset shift. You're not replacing monitoring—you're adding a prevention layer that blocks non-compliant code before deployment. This is the difference between \"catching problems after damage\" and \"preventing problems before deployment.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check #2: Conceptual Understanding\nprint(\"Check #2: Conceptual Understanding (Detective vs. Preventive)\")\nprint(\"=\" * 50)\n\n# Conceptual readiness questions\nquestions = [\n    \"Q1: What's the difference between detective controls and preventive controls?\",\n    \"    (Detective = catch violations after deployment; Preventive = block violations before deployment)\",\n    \"\",\n    \"Q2: Why can't you rely on monitoring alone for SOC 2 Type II?\",\n    \"    (SOC 2 requires preventive controls, not just detective controls)\",\n    \"\",\n    \"Q3: In the bridge example, why was a 9-hour detection window catastrophic?\",\n    \"    (847 users already downloaded unredacted PII; damage was done before detection)\",\n    \"\",\n    \"Q4: How do CI/CD testing gates prevent violations?\",\n    \"    (Automated tests run on every commit; deployments blocked if tests fail)\"\n]\n\nprint(\"\\n✓ Review these concepts:\\n\")\nfor q in questions:\n    print(f\"   {q}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"✓ Check #2 PASS if you can clearly explain all 4 concepts\")\nprint(\"✗ Check #2 FAIL if conceptual gaps remain\")\nprint(\"\\n   Fix: Review bridge video and M3.1→M3.2 transition materials\")\n\n# Expected: User validates conceptual understanding",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Readiness Check #3: Stakeholder Perspective Understanding\n\n**What This Validates:** You understand why CFOs, Compliance Officers, and CTOs demand preventive controls, not just monitoring\n\n**Pass Criteria:**\n- ✓ You can articulate the CFO's concern about personal liability (signing inaccurate SOX 404 certifications)\n- ✓ You understand the Compliance Officer's regulatory requirement for preventive controls (not just detective)\n- ✓ You grasp the CTO's need for automated guardrails that scale to 500 engineers deploying 50x/day\n\n**Why This Matters:** M3.2 isn't just a technical upgrade—it's addressing critical business and regulatory requirements. Understanding stakeholder perspectives helps you build the right solution and communicate its value.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check #3: Stakeholder Perspective Understanding\nprint(\"Check #3: Stakeholder Perspective Understanding\")\nprint(\"=\" * 50)\n\n# Stakeholder concerns from bridge script\nstakeholders = {\n    \"CFO\": \"Personal liability signing SOX 404 certifications quarterly - if monitoring catches violations after deployment, CFO has already signed inaccurate certifications to SEC\",\n    \"Compliance Officer\": \"Regulators require PREVENTIVE controls, not just DETECTIVE controls - SOC 2 CC6.1 specifically demands prevention, not just detection\",\n    \"CTO\": \"Can't rely on tribal knowledge at scale - need automated guardrails that run on every commit for 500 engineers deploying 50x/day across 50 tenants\"\n}\n\nprint(\"\\n✓ Can you articulate these stakeholder concerns?\\n\")\nfor role, concern in stakeholders.items():\n    print(f\"   {role}:\")\n    print(f\"   {concern}\\n\")\n\nprint(\"=\" * 50)\nprint(\"✓ Check #3 PASS if you understand all 3 perspectives\")\nprint(\"✗ Check #3 FAIL if you can't explain stakeholder concerns\")\nprint(\"\\n   Fix: Review bridge Section 2 (stakeholder perspectives)\")\n\n# Expected: User validates stakeholder understanding",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Readiness Check #4: CI/CD and Policy-as-Code Readiness\n\n**What This Validates:** You have the foundational knowledge to integrate automated testing into CI/CD pipelines\n\n**Pass Criteria:**\n- ✓ You understand what CI/CD pipelines are (GitHub Actions, GitLab CI, Jenkins, etc.)\n- ✓ You grasp the concept of \"policy-as-code\" (compliance rules written as executable code)\n- ✓ You know what a \"testing gate\" means (tests that must pass before deployment proceeds)\n- ✓ You understand how OPA (Open Policy Agent) fits into compliance automation\n\n**Why This Matters:** M3.2 uses OPA Rego to write compliance policies as code, then integrates those tests into CI/CD pipelines. You don't need to be an expert yet—but you should understand the basic concepts before diving into implementation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check #4: CI/CD and Policy-as-Code Readiness\nprint(\"Check #4: CI/CD and Policy-as-Code Readiness\")\nprint(\"=\" * 50)\n\n# Foundational concepts for M3.2\nconcepts = [\n    \"1. CI/CD Pipeline: Automated workflow that runs tests on every code commit\",\n    \"   Examples: GitHub Actions, GitLab CI, Jenkins\",\n    \"\",\n    \"2. Policy-as-Code: Compliance rules written as executable code (not docs)\",\n    \"   Example: OPA Rego policy that tests 'PII detection covers 100% of text paths'\",\n    \"\",\n    \"3. Testing Gate: Checkpoint in CI/CD that blocks deployment if tests fail\",\n    \"   Example: 'Deployment cannot proceed if compliance tests show failures'\",\n    \"\",\n    \"4. OPA (Open Policy Agent): Industry-standard tool for policy-as-code\",\n    \"   Usage: Write Rego policies, run automated tests, block non-compliant deployments\"\n]\n\nprint(\"\\n✓ Verify you understand these concepts:\\n\")\nfor concept in concepts:\n    print(f\"   {concept}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"✓ Check #4 PASS if you understand all 4 concepts\")\nprint(\"✗ Check #4 FAIL if concepts are unfamiliar\")\nprint(\"\\n   Fix: Review CI/CD and OPA introduction materials before M3.2\")\n\n# Expected: User validates CI/CD and policy-as-code understanding",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Readiness Check #5: Career and Impact Understanding\n\n**What This Validates:** You understand the career value and real-world impact of M3.2's automated testing capabilities\n\n**Pass Criteria:**\n- ✓ You know the salary range for compliance automation engineers at GCCs (₹18-28L)\n- ✓ You understand the audit prep time reduction (8 hours → 30 minutes)\n- ✓ You can articulate the prevention rate (95%+ violations blocked before production)\n- ✓ You grasp why both monitoring (M3.1) and prevention (M3.2) are required for SOC 2 Type II certification\n\n**Why This Matters:** Understanding the business value and career positioning helps you stay motivated through M3.2's technical implementation. You're not just learning OPA Rego—you're building skills that unlock ₹18-28L roles at financial services GCCs, healthcare GCCs, and regulated SaaS companies.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check #5: Career and Impact Understanding\nprint(\"Check #5: Career and Impact Understanding\")\nprint(\"=\" * 50)\n\n# Real-world impact metrics from bridge script\nimpact_metrics = {\n    \"Salary Range\": \"₹18-28L for compliance automation engineers at GCCs\",\n    \"Audit Prep Time\": \"8 hours → 30 minutes (automated evidence generation)\",\n    \"Prevention Rate\": \"95%+ violations blocked before production\",\n    \"SOC 2 Requirement\": \"Both monitoring (detective) + testing (preventive) required for Type II\"\n}\n\nprint(\"\\n✓ Key metrics you should know:\\n\")\nfor metric, value in impact_metrics.items():\n    print(f\"   {metric}: {value}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"✓ Check #5 PASS if you understand the business value and career impact\")\nprint(\"✗ Check #5 FAIL if you can't articulate why M3.2 matters\")\nprint(\"\\n   Fix: Review bridge Section 5 (career positioning)\")\n\n# Expected: User validates career and impact understanding",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Call-Forward: What's Next in M3.2\n\n**Module M3.2 Will Cover:**\n\n**Core Framework:**\n- **Policy-as-Code with OPA Rego** — Write compliance rules as executable code (not documentation)\n- **16+ Automated Compliance Tests:**\n  - PII detection coverage (100% of text paths tested)\n  - Access control enforcement (zero cross-tenant leaks verified)\n  - Audit logging completeness (>99.5% coverage)\n  - Data retention policies (SOX 7-year + GDPR deletion verified)\n- **CI/CD Integration with Pass/Fail Gates** — Tests run automatically on every commit; deployments blocked if any test fails\n- **Regression Prevention** — Ensure working controls stay working\n\n**Real-World Impact You'll Achieve:**\n- **95%+ compliance violations prevented** before production deployment\n- **Audit prep time reduced** from 8 hours → 30 minutes (automated evidence export)\n- **Developer confidence restored** — Deploy without fear of breaking compliance unknowingly\n- **CFO/Compliance trust rebuilt** — \"Engineering has automated preventive controls, not just monitoring\"\n\n**Why You're Ready:**\n\nYou've completed M3.1's monitoring layer—the foundation that provides real-time visibility into compliance violations. M3.2 builds on this by adding the prevention layer that blocks violations **before** they reach production.\n\nTogether, these two layers create the defense-in-depth approach that SOC 2 Type II requires:\n- **Prevention Layer (M3.2):** Blocks 95%+ of violations before deployment\n- **Detection Layer (M3.1):** Catches the remaining 5% that slip through\n\n**What to Expect in M3.2:**\n\n- **Duration:** 90-120 minutes (hands-on PractaThon)\n- **Complexity:** Moderate—you'll write OPA Rego policies and integrate them into CI/CD\n- **Key Deliverables:**\n  - Production-ready OPA testing suite (16+ tests)\n  - GitHub Actions/GitLab CI integration\n  - Automated compliance evidence generation\n  - Regression prevention framework\n\n**If You're Not Ready:**\n\n- Review M3.1 materials and ensure your monitoring dashboard is operational\n- Complete any failed checks above\n- Understand the conceptual difference between detective and preventive controls\n- Reach out for support: support@techvoyagehub.com\n\n**Next Steps:**\n\n1. ✅ Ensure ALL 5 checks passed (✓)\n2. ✅ Verify your monitoring infrastructure from M3.1 is operational\n3. → **Proceed to M3.2: Automated Compliance Testing**\n4. → Start building your policy-as-code framework with OPA Rego\n\n**Career Positioning:**\n\nBy completing M3.1 + M3.2, you'll have both layers of the compliance automation stack:\n- **Detection (M3.1):** Monitoring dashboards that catch violations after deployment\n- **Prevention (M3.2):** Automated tests that block violations before deployment\n\nThis combination unlocks ₹18-28L roles at:\n- Financial services GCCs (SOX + SOC 2 required)\n- Healthcare GCCs (HIPAA + SOC 2 required)\n- Regulated SaaS companies (compliance automation teams)\n\nYou're building the exact testing frameworks that pass SOC 2 audits. Let's continue!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}