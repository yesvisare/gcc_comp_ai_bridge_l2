{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "## Run Locally (Windows)\n",
    "\n",
    "```powershell\n",
    "$env:PYTHONPATH = \"$PWD\"\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "px2iflho0a8",
   "source": "## 1. Purpose\n\n**What Shifts:**\n- From: M3.3 \u2014 Audit Logging & SIEM Integration\n- To: M3.4 \u2014 Incident Response & Breach Notification\n\n**Why This Bridge Matters:**\n\nM3.3 established comprehensive audit trails: PostgreSQL Row-Level Security capturing every data access, S3 Object Lock ensuring tamper-proof retention, and Splunk forwarding events in real-time. You can now detect incidents immediately through anomaly detection rules (bulk exports, after-hours access).\n\nBut detection alone is insufficient for production compliance systems.\n\n**The Gap:** A March 2024 European fintech breach illustrates the critical issue\u2014compromised credentials exposed 12,000 customer records. Audit logs detected the breach immediately, but manual response processes delayed GDPR Article 33 notification beyond the 72-hour requirement, resulting in a \u20ac4.2M fine (\u20b938 crores).\n\n**M3.4 closes this detection-to-response gap** by building automated incident response workflows that execute documented procedures from classification through notification, turning your audit infrastructure into a compliance-ready incident response system.\n\n**Bridge Type:** Readiness Validation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "s6n2pbev0pg",
   "source": "## 2. Concepts Covered\n\n**New Concepts in M3.4:**\n\n- **4-Tier Incident Classification (P0-P3 Severity Matrix)** \u2014 Standardized framework for categorizing incident severity in under 30 seconds, enabling appropriate response resource allocation\n- **6-Phase Response Workflow** \u2014 Detection \u2192 Containment \u2192 Eradication \u2192 Recovery \u2192 Notification \u2192 Post-Mortem; structured process with defined actions, responsible parties, and completion criteria\n- **GDPR Article 33 Automation** \u2014 72-hour notification templates with programmatic data population from PostgreSQL audit logs\n- **DPDPA Notification Engine** \u2014 Automated Indian supervisory authority notification for cross-border compliance\n- **Root Cause Analysis Tools** \u2014 Audit log correlation for end-to-end incident timeline reconstruction using correlation IDs\n- **Remediation Tracking Dashboard** \u2014 Closure verification system tracking remediation steps from detection through completion with audit-ready post-mortem reports\n\n**Building On:**\n- M3.3 established: Real-time audit trails, tamper-proof retention (S3 Object Lock), SIEM forwarding (Splunk), anomaly detection\n- M3.4 extends: Automated response workflows that answer \"who was affected?\" and \"what data exposed?\" programmatically, transforming detection infrastructure into compliance-ready incident response system",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ubmhqlalll",
   "source": "## 3. After Completing This Bridge\n\n**You Will Be Able To:**\n\n- \u2713 Verify PostgreSQL audit tables with Row-Level Security are operational and capturing all data access events\n- \u2713 Confirm S3 Object Lock is configured in COMPLIANCE mode with 7-10 year retention for tamper-proof audit trail preservation\n- \u2713 Validate Splunk Universal Forwarder is actively forwarding audit events in real-time to SIEM infrastructure\n- \u2713 Assess Hot/Warm/Cold storage tier implementation for cost-optimized audit data lifecycle management\n- \u2713 Test anomaly detection rules (bulk export, after-hours access) are deployed and generating alerts\n- \u2713 Understand readiness requirements for building automated incident response workflows in M3.4\n\n**Pass Criteria:**\n- All 5 checks pass (\u2713)\n- No critical gaps (\u2717) in M3.3 audit infrastructure\n- Ready for M3.4 incident response automation content",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "7zs5vfc3y4t",
   "source": "## 4. Context in Track\n\n**Position:** Bridge L3.M3.3 \u2192 L3.M3.4\n\n**Learning Journey:**\n```\nL3.M3.3 \u2500\u2500\u2500\u2500[THIS BRIDGE]\u2500\u2500\u2500\u2192 L3.M3.4\nAudit Logging      Validation      Incident Response\n& SIEM Integration              & Breach Notification\n```\n\n**Compliance Infrastructure Continuity:**\n- M3.1: Monitoring \u2192 Real-time observability foundations\n- M3.2: Testing \u2192 Compliance verification frameworks\n- M3.3: Audit Trails \u2192 Tamper-proof evidence collection\n- **M3.4: Response Automation** \u2190 Closes detection-to-notification gap\n- Complete production-grade compliance chain\n\n**Time Estimate:** 15-30 minutes",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "6954n9qeeni",
   "source": "## Recap: What You Built in M3.3\n\nIn M3.3, you shipped a **production-grade audit logging and SIEM integration system** that established comprehensive audit trails for GCC compliance.\n\n**Key Deliverables:**\n- **PostgreSQL Audit Tables with Row-Level Security** \u2014 Every data access captured with user context, session tracking, and tamper-proof RLS policies\n- **S3 Object Lock (COMPLIANCE Mode)** \u2014 Immutable audit trail storage with 7-10 year retention, preventing modification or deletion even by administrators\n- **Splunk Universal Forwarder** \u2014 Real-time audit event forwarding to SIEM infrastructure for centralized monitoring and correlation\n- **Hot/Warm/Cold Storage Tiers** \u2014 Cost-optimized audit data lifecycle management balancing compliance requirements with storage economics\n- **Anomaly Detection Rules** \u2014 Automated alerts for suspicious patterns (bulk export attempts, after-hours access) with correlation IDs for incident investigation\n\n**What This Enables:**\nYou can now **detect** compliance incidents immediately through real-time monitoring. The next critical capability is **automated response**\u2014M3.4 transforms detection into documented, compliant incident response workflows.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "i4c3xrxqacp",
   "source": "## Readiness Check #1: PostgreSQL Audit Tables with Row-Level Security\n\n**What This Validates:** Confirms that PostgreSQL audit tables are operational with Row-Level Security (RLS) policies actively capturing all data access events with user context.\n\n**Pass Criteria:**\n- \u2713 Audit table schema exists with required columns (timestamp, user_id, action, resource, session_id, correlation_id)\n- \u2713 Row-Level Security policies are enabled and preventing unauthorized access\n- \u2713 Recent audit events are being captured (data within last 24 hours)\n- \u2713 User context is properly recorded (no NULL user_id values)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wztk97piffd",
   "source": "# Check 1: PostgreSQL Audit Tables with Row-Level Security\nimport os\nfrom pathlib import Path\n\n# Offline-friendly guard\nDB_CREDENTIALS = os.getenv(\"POSTGRES_CONNECTION_STRING\")\n\nif not DB_CREDENTIALS:\n    print(\"\u26a0\ufe0f Skipping (no PostgreSQL credentials)\")\n    print(\"   Set: POSTGRES_CONNECTION_STRING environment variable\")\nelse:\n    # Check for audit table artifacts from M3.3\n    audit_config = Path(\"config/audit_tables.sql\")\n    rls_policies = Path(\"config/rls_policies.sql\")\n    \n    checks_passed = []\n    \n    if audit_config.exists():\n        print(\"\u2713 Audit table schema configuration found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/audit_tables.sql\")\n        print(\"   Fix: Run M3.3 to generate audit table schema\")\n        checks_passed.append(False)\n    \n    if rls_policies.exists():\n        print(\"\u2713 RLS policy configuration found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/rls_policies.sql\")\n        print(\"   Fix: Run M3.3 to generate RLS policies\")\n        checks_passed.append(False)\n    \n    if all(checks_passed):\n        print(\"\\n\u2713 Check #1 PASSED - PostgreSQL audit infrastructure ready\")\n    else:\n        print(\"\\n\u2717 Check #1 FAILED - Complete M3.3 audit table setup\")\n\n# Expected: \u2713 Check #1 PASSED - PostgreSQL audit infrastructure ready",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "si1la40o06",
   "source": "## Readiness Check #2: S3 Object Lock Configuration\n\n**What This Validates:** Verifies that S3 Object Lock is configured in COMPLIANCE mode with appropriate retention periods for tamper-proof audit trail storage.\n\n**Pass Criteria:**\n- \u2713 S3 bucket exists with Object Lock enabled\n- \u2713 COMPLIANCE mode configured (not GOVERNANCE mode)\n- \u2713 Retention period set to 7-10 years for regulatory compliance\n- \u2713 Bucket versioning enabled (required for Object Lock)\n- \u2713 Configuration documented for audit verification",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "auta71ovrcc",
   "source": "# Check 2: S3 Object Lock Configuration\nimport os\nfrom pathlib import Path\n\n# Offline-friendly guard\nAWS_CREDENTIALS = os.getenv(\"AWS_ACCESS_KEY_ID\") and os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n\nif not AWS_CREDENTIALS:\n    print(\"\u26a0\ufe0f Skipping (no AWS credentials)\")\n    print(\"   Set: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables\")\nelse:\n    # Check for S3 Object Lock configuration artifacts from M3.3\n    s3_config = Path(\"config/s3_object_lock.json\")\n    bucket_policy = Path(\"config/s3_audit_bucket_policy.json\")\n    \n    checks_passed = []\n    \n    if s3_config.exists():\n        print(\"\u2713 S3 Object Lock configuration found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/s3_object_lock.json\")\n        print(\"   Fix: Run M3.3 to configure S3 Object Lock\")\n        checks_passed.append(False)\n    \n    if bucket_policy.exists():\n        print(\"\u2713 S3 bucket policy configuration found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/s3_audit_bucket_policy.json\")\n        print(\"   Fix: Run M3.3 to configure S3 bucket policies\")\n        checks_passed.append(False)\n    \n    if all(checks_passed):\n        print(\"\\n\u2713 Check #2 PASSED - S3 Object Lock ready for tamper-proof storage\")\n    else:\n        print(\"\\n\u2717 Check #2 FAILED - Complete M3.3 S3 Object Lock setup\")\n\n# Expected: \u2713 Check #2 PASSED - S3 Object Lock ready for tamper-proof storage",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0z6wh13waof",
   "source": "## Readiness Check #3: Splunk Universal Forwarder Verification\n\n**What This Validates:** Confirms that Splunk Universal Forwarder is configured and actively forwarding audit events in real-time to SIEM infrastructure.\n\n**Pass Criteria:**\n- \u2713 Splunk Universal Forwarder installed and configured\n- \u2713 Forwarder connected to Splunk indexers (deployment server configured)\n- \u2713 Audit log inputs configured (PostgreSQL, application logs)\n- \u2713 Real-time forwarding active (no significant lag)\n- \u2713 Forwarding health monitored (connection status, throughput)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ttihzg8nz",
   "source": "# Check 3: Splunk Universal Forwarder Verification\nimport os\nfrom pathlib import Path\n\n# Offline-friendly guard\nSPLUNK_CONFIGURED = os.getenv(\"SPLUNK_FORWARDER_CONFIGURED\")\n\nif not SPLUNK_CONFIGURED:\n    print(\"\u26a0\ufe0f Skipping (no Splunk configuration)\")\n    print(\"   Set: SPLUNK_FORWARDER_CONFIGURED=true after M3.3 setup\")\nelse:\n    # Check for Splunk forwarder configuration artifacts from M3.3\n    forwarder_config = Path(\"config/splunk_forwarder_inputs.conf\")\n    outputs_config = Path(\"config/splunk_forwarder_outputs.conf\")\n    \n    checks_passed = []\n    \n    if forwarder_config.exists():\n        print(\"\u2713 Splunk forwarder inputs configuration found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/splunk_forwarder_inputs.conf\")\n        print(\"   Fix: Run M3.3 to configure Splunk forwarder inputs\")\n        checks_passed.append(False)\n    \n    if outputs_config.exists():\n        print(\"\u2713 Splunk forwarder outputs configuration found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/splunk_forwarder_outputs.conf\")\n        print(\"   Fix: Run M3.3 to configure Splunk forwarder outputs\")\n        checks_passed.append(False)\n    \n    if all(checks_passed):\n        print(\"\\n\u2713 Check #3 PASSED - Splunk forwarder ready for real-time SIEM integration\")\n    else:\n        print(\"\\n\u2717 Check #3 FAILED - Complete M3.3 Splunk forwarder setup\")\n\n# Expected: \u2713 Check #3 PASSED - Splunk forwarder ready for real-time SIEM integration",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3ub1i5islvn",
   "source": "## Readiness Check #4: Hot/Warm/Cold Storage Tier Implementation\n\n**What This Validates:** Verifies that storage tiering is implemented for cost-optimized audit data lifecycle management while maintaining compliance requirements.\n\n**Pass Criteria:**\n- \u2713 Hot tier configured for recent data (0-30 days, high-performance access)\n- \u2713 Warm tier configured for medium-term data (31-365 days, standard access)\n- \u2713 Cold tier configured for long-term retention (1-10 years, archival access)\n- \u2713 Lifecycle policies automate tier transitions\n- \u2713 Retrieval times documented for compliance audit requirements",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xs8jrpxglde",
   "source": "# Check 4: Hot/Warm/Cold Storage Tier Implementation\nimport os\nfrom pathlib import Path\n\n# Offline-friendly guard\nSTORAGE_TIERS_CONFIGURED = os.getenv(\"STORAGE_TIERS_CONFIGURED\")\n\nif not STORAGE_TIERS_CONFIGURED:\n    print(\"\u26a0\ufe0f Skipping (no storage tier configuration)\")\n    print(\"   Set: STORAGE_TIERS_CONFIGURED=true after M3.3 setup\")\nelse:\n    # Check for storage tier configuration artifacts from M3.3\n    lifecycle_policy = Path(\"config/s3_lifecycle_policy.json\")\n    tier_documentation = Path(\"docs/storage_tier_architecture.md\")\n    \n    checks_passed = []\n    \n    if lifecycle_policy.exists():\n        print(\"\u2713 S3 lifecycle policy configuration found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/s3_lifecycle_policy.json\")\n        print(\"   Fix: Run M3.3 to configure storage tier lifecycle policies\")\n        checks_passed.append(False)\n    \n    if tier_documentation.exists():\n        print(\"\u2713 Storage tier documentation found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: docs/storage_tier_architecture.md\")\n        print(\"   Fix: Run M3.3 to document storage tier architecture\")\n        checks_passed.append(False)\n    \n    if all(checks_passed):\n        print(\"\\n\u2713 Check #4 PASSED - Storage tiers ready for cost-optimized compliance\")\n    else:\n        print(\"\\n\u2717 Check #4 FAILED - Complete M3.3 storage tier implementation\")\n\n# Expected: \u2713 Check #4 PASSED - Storage tiers ready for cost-optimized compliance",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "m5leyginys",
   "source": "## Readiness Check #5: Anomaly Detection Rules Deployment\n\n**What This Validates:** Confirms that anomaly detection rules are deployed in Splunk to identify suspicious patterns requiring incident response.\n\n**Pass Criteria:**\n- \u2713 Bulk export detection rule deployed (threshold: >1000 records in single query)\n- \u2713 After-hours access detection rule deployed (non-business hours data access)\n- \u2713 Rules generate alerts with correlation IDs for incident tracking\n- \u2713 Alert routing configured (email, ticketing system, dashboards)\n- \u2713 False positive tuning documented for operational efficiency",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yddk6nag8od",
   "source": "# Check 5: Anomaly Detection Rules Deployment\nimport os\nfrom pathlib import Path\n\n# Offline-friendly guard\nANOMALY_RULES_DEPLOYED = os.getenv(\"ANOMALY_RULES_DEPLOYED\")\n\nif not ANOMALY_RULES_DEPLOYED:\n    print(\"\u26a0\ufe0f Skipping (no anomaly detection rules configured)\")\n    print(\"   Set: ANOMALY_RULES_DEPLOYED=true after M3.3 setup\")\nelse:\n    # Check for anomaly detection rule artifacts from M3.3\n    bulk_export_rule = Path(\"config/splunk_alerts/bulk_export_detection.spl\")\n    after_hours_rule = Path(\"config/splunk_alerts/after_hours_access.spl\")\n    alert_routing = Path(\"config/splunk_alerts/alert_routing.conf\")\n    \n    checks_passed = []\n    \n    if bulk_export_rule.exists():\n        print(\"\u2713 Bulk export detection rule found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/splunk_alerts/bulk_export_detection.spl\")\n        print(\"   Fix: Run M3.3 to deploy bulk export detection rule\")\n        checks_passed.append(False)\n    \n    if after_hours_rule.exists():\n        print(\"\u2713 After-hours access detection rule found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/splunk_alerts/after_hours_access.spl\")\n        print(\"   Fix: Run M3.3 to deploy after-hours access detection rule\")\n        checks_passed.append(False)\n    \n    if alert_routing.exists():\n        print(\"\u2713 Alert routing configuration found\")\n        checks_passed.append(True)\n    else:\n        print(\"\u2717 Missing: config/splunk_alerts/alert_routing.conf\")\n        print(\"   Fix: Run M3.3 to configure alert routing\")\n        checks_passed.append(False)\n    \n    if all(checks_passed):\n        print(\"\\n\u2713 Check #5 PASSED - Anomaly detection ready for incident response\")\n    else:\n        print(\"\\n\u2717 Check #5 FAILED - Complete M3.3 anomaly detection deployment\")\n\n# Expected: \u2713 Check #5 PASSED - Anomaly detection ready for incident response",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ydsnjjs4sng",
   "source": "## Call-Forward: What's Next in M3.4\n\n**The Driving Question:**\n\n*\"Your RAG system exposed 10,000 customer records; audit logs detected it immediately. You have 72 hours to comply with GDPR Article 33 and DPDPA breach notification requirements. What do you do in the next 72 hours?\"*\n\n---\n\n**Module M3.4 Will Cover:**\n\n**1. Production-Grade Incident Response System**\n- Build automated workflows that transform audit trail detection into compliant response execution\n- Close the detection-to-response gap that caused the \u20ac4.2M fintech fine\n\n**2. 4-Tier Incident Classification (P0-P3 Severity Matrix)**\n- Classify incidents in under 30 seconds using standardized severity framework\n- Map detected events from audit logs to appropriate response resource allocation\n\n**3. 6-Phase Response Workflow Automation**\n- Detection \u2192 Containment \u2192 Eradication \u2192 Recovery \u2192 Notification \u2192 Post-Mortem\n- Structured processes with defined actions, responsible parties, and completion criteria\n- End-to-end incident timeline reconstruction using correlation IDs\n\n**4. Regulatory Compliance Automation**\n- **GDPR Article 33:** Generate 72-hour notifications with programmatic data population\n- **DPDPA:** Automate Indian supervisory authority notifications for cross-border compliance\n- Pre-built SQL queries answering \"who was affected?\" and \"what data exposed?\" in minutes\n\n**5. Remediation Tracking & Post-Mortem**\n- Track remediation steps from detection through verified closure\n- Generate audit-ready post-mortem reports with timeline correlation\n\n---\n\n**Why You're Ready:**\n\nYour M3.3 audit infrastructure provides the **foundation** for incident response:\n- PostgreSQL audit tables capture who, what, when for every data access\n- S3 Object Lock ensures tamper-proof evidence preservation\n- Splunk forwarding enables real-time correlation and timeline reconstruction\n- Anomaly detection identifies incidents requiring response\n\nM3.4 adds the **response automation layer** that transforms detection into documented, compliant action.\n\n---\n\n**What to Expect:**\n\n- **Duration:** 4-5 hours (PractaThon + Conceptual)\n- **Complexity:** Production-grade compliance automation (beyond MVP/hobby projects)\n- **Key Deliverables:**\n  - Incident classification system with severity matrix\n  - 6-phase response workflow automation\n  - GDPR/DPDPA notification templates with programmatic data population\n  - Remediation tracking dashboard\n  - Post-mortem report generator\n\n---\n\n**Career Positioning:**\n\nCompleting M3.4 positions you for **\u20b918-28 lakh GCC Compliance Engineer or Security Operations Engineer roles** by demonstrating end-to-end compliance infrastructure ownership, not isolated happy-path scenarios.\n\n**Production Differentiator:**\n> \"Hobby projects detect incidents and panic. Production systems detect incidents and execute documented response workflows automatically.\"\n\n---\n\n**If You're Not Ready:**\n\n- Review M3.3 materials (especially audit logging and SIEM integration)\n- Complete failed checks above\n- Ensure audit infrastructure is operational\n- Reach out for support: support@techvoyagehub.com\n\n---\n\n**Next Steps:**\n\n1. \u2705 Ensure ALL 5 checks passed (\u2713)\n2. \u2192 Proceed to **M3.4: Incident Response & Breach Notification**\n3. \ud83d\udccc Reference this bridge if stuck during M3.4",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}